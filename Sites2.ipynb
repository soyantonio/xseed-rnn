{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import chardet\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus: \n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "dictionary = set()\n",
    "text = ''\n",
    "\n",
    "for i in range(15):\n",
    "    for j in range(49):\n",
    "        path_to_file = '/home/jupyter/data/g' + str(i) + '/d'+str(j)+'.txt'\n",
    "            \n",
    "        file = open(path_to_file, \"rb\")\n",
    "        rawdata = file.read()\n",
    "        file.close()\n",
    "        result = chardet.detect(rawdata)\n",
    "        \n",
    "        try:\n",
    "            decoded=rawdata.decode(encoding=result['encoding'])\n",
    "        except:\n",
    "            None\n",
    "        else:\n",
    "            if result['encoding'] == \"utf-8\":\n",
    "                formated = decoded.strip()\n",
    "                formated = formated.replace('\\r', '')\n",
    "                #texts.append(formated)\n",
    "                text += formated\n",
    "                dictionary = dictionary.union(set(formated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(dictionary)\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary elements 2166\n",
      "Documents # 0\n",
      "Text characters # 25678937\n"
     ]
    }
   ],
   "source": [
    "#int_texts = []\n",
    "#for txt in texts:\n",
    "#    int_texts.append(np.array([char2idx[c] for c in txt]))\n",
    "int_text = np.array([char2idx[c] for c in text])\n",
    "\n",
    "print(\"Vocabulary elements\", len(vocab))\n",
    "print(\"Documents #\", len(texts))\n",
    "print(\"Text characters #\", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequences(txt_int):\n",
    "    # Create training examples / targets\n",
    "    seq_length = 80\n",
    "    char_dataset = tf.data.Dataset.from_tensor_slices(txt_int)\n",
    "    sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "    return sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================== Checkpoint ==============================================\n",
    "# ============================================== Checkpoint ==============================================\n",
    "# The maximum length sentence\n",
    "# datasets = []\n",
    "\n",
    "dataset = get_sequences(int_text)\n",
    "\n",
    "#for int_text in int_texts:\n",
    "#    datasets.append(get_sequences(int_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  '<html>\\n<head>\\n<meta charset=\"UTF-8\"/>\\n\\n<!--stops zooming-->\\n<meta name=\"viewport'\n",
      "Target data: 'html>\\n<head>\\n<meta charset=\"UTF-8\"/>\\n\\n<!--stops zooming-->\\n<meta name=\"viewport\"'\n"
     ]
    }
   ],
   "source": [
    "# for input_example, target_example in  datasets[0].take(1):\n",
    "for input_example, target_example in  dataset.take(1):    \n",
    "    print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "    print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# Buffer size to shuffle the dataset. Maintains a buffer in which it shuffles elements), not all dataset.\n",
    "BUFFER_SIZE = 10000\n",
    "#BUFFER_SIZE = 500\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "#for i in range(len(datasets)):\n",
    "#    datasets[i] = datasets[i].shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                                  batch_input_shape=[batch_size, None]),\n",
    "        tf.keras.layers.GRU(rnn_units,\n",
    "                            return_sequences=True,\n",
    "                            stateful=True,\n",
    "                            recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = vocab_size,\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, target):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inp)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.keras.losses.sparse_categorical_crossentropy(\n",
    "                target, predictions, from_logits=True))\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 7.679959297180176\n",
      "Epoch 1 Batch 200 Loss 2.551133632659912\n",
      "Epoch 1 Batch 400 Loss 2.0129566192626953\n",
      "Epoch 1 Batch 600 Loss 1.8143341541290283\n",
      "Epoch 1 Batch 800 Loss 1.466618299484253\n",
      "Epoch 1 Batch 1000 Loss 1.2992297410964966\n",
      "Epoch 1 Batch 1200 Loss 0.9799860119819641\n",
      "Epoch 1 Batch 1400 Loss 1.2368547916412354\n",
      "Epoch 1 Batch 1600 Loss 1.266495704650879\n",
      "Epoch 1 Batch 1800 Loss 1.1634080410003662\n",
      "Epoch 1 Batch 2000 Loss 1.246417760848999\n",
      "Epoch 1 Batch 2200 Loss 0.9877511858940125\n",
      "Epoch 1 Batch 2400 Loss 1.0658996105194092\n",
      "Epoch 1 Loss 1.0278\n",
      "Time taken for epoch 329.87599420547485 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 6.630316257476807\n",
      "Epoch 2 Batch 200 Loss 1.2034952640533447\n",
      "Epoch 2 Batch 400 Loss 1.0775505304336548\n",
      "Epoch 2 Batch 600 Loss 1.122586727142334\n",
      "Epoch 2 Batch 800 Loss 1.0678249597549438\n",
      "Epoch 2 Batch 1000 Loss 1.0100433826446533\n",
      "Epoch 2 Batch 1200 Loss 0.8078562617301941\n",
      "Epoch 2 Batch 1400 Loss 0.9690583348274231\n",
      "Epoch 2 Batch 1600 Loss 1.059451699256897\n",
      "Epoch 2 Batch 1800 Loss 1.0719257593154907\n",
      "Epoch 2 Batch 2000 Loss 1.0628185272216797\n",
      "Epoch 2 Batch 2200 Loss 0.9403400421142578\n",
      "Epoch 2 Batch 2400 Loss 1.0290583372116089\n",
      "Epoch 2 Loss 0.9219\n",
      "Time taken for epoch 332.65412616729736 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.2495311498641968\n",
      "Epoch 3 Batch 200 Loss 0.8981634378433228\n",
      "Epoch 3 Batch 400 Loss 1.0063495635986328\n",
      "Epoch 3 Batch 600 Loss 1.005767822265625\n",
      "Epoch 3 Batch 800 Loss 1.0868384838104248\n",
      "Epoch 3 Batch 1000 Loss 1.082332968711853\n",
      "Epoch 3 Batch 1200 Loss 0.9320619702339172\n",
      "Epoch 3 Batch 1400 Loss 1.000623106956482\n",
      "Epoch 3 Batch 1600 Loss 0.9859892129898071\n",
      "Epoch 3 Batch 1800 Loss 0.9951979517936707\n",
      "Epoch 3 Batch 2000 Loss 1.0236302614212036\n",
      "Epoch 3 Batch 2200 Loss 0.8431441187858582\n",
      "Epoch 3 Batch 2400 Loss 0.8886896371841431\n",
      "Epoch 3 Loss 0.8505\n",
      "Time taken for epoch 333.64189314842224 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 1.238521933555603\n",
      "Epoch 4 Batch 200 Loss 0.921211838722229\n",
      "Epoch 4 Batch 400 Loss 1.029087781906128\n",
      "Epoch 4 Batch 600 Loss 0.9848343133926392\n",
      "Epoch 4 Batch 800 Loss 0.9567344784736633\n",
      "Epoch 4 Batch 1000 Loss 0.8988819122314453\n",
      "Epoch 4 Batch 1200 Loss 0.7718949913978577\n",
      "Epoch 4 Batch 1400 Loss 0.9524949789047241\n",
      "Epoch 4 Batch 1600 Loss 1.064434289932251\n",
      "Epoch 4 Batch 1800 Loss 1.0018515586853027\n",
      "Epoch 4 Batch 2000 Loss 0.9471017122268677\n",
      "Epoch 4 Batch 2200 Loss 0.8143280744552612\n",
      "Epoch 4 Batch 2400 Loss 1.1862777471542358\n",
      "Epoch 4 Loss 1.0084\n",
      "Time taken for epoch 333.97902154922485 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 1.2175203561782837\n",
      "Epoch 5 Batch 200 Loss 0.9135257601737976\n",
      "Epoch 5 Batch 400 Loss 0.9384914636611938\n",
      "Epoch 5 Batch 600 Loss 0.9133800268173218\n",
      "Epoch 5 Batch 800 Loss 0.930087685585022\n",
      "Epoch 5 Batch 1000 Loss 0.8956352472305298\n",
      "Epoch 5 Batch 1200 Loss 0.7337600588798523\n",
      "Epoch 5 Batch 1400 Loss 0.9987356066703796\n",
      "Epoch 5 Batch 1600 Loss 0.9441171884536743\n",
      "Epoch 5 Batch 1800 Loss 1.0101392269134521\n",
      "Epoch 5 Batch 2000 Loss 0.9783790707588196\n",
      "Epoch 5 Batch 2200 Loss 0.7718841433525085\n",
      "Epoch 5 Batch 2400 Loss 0.9377702474594116\n",
      "Epoch 5 Loss 0.9369\n",
      "Time taken for epoch 333.1351020336151 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 1.1864079236984253\n",
      "Epoch 6 Batch 200 Loss 0.9314143061637878\n",
      "Epoch 6 Batch 400 Loss 0.9380409121513367\n",
      "Epoch 6 Batch 600 Loss 0.9164242744445801\n",
      "Epoch 6 Batch 800 Loss 0.9393705129623413\n",
      "Epoch 6 Batch 1000 Loss 0.875113308429718\n",
      "Epoch 6 Batch 1200 Loss 0.8203216791152954\n",
      "Epoch 6 Batch 1400 Loss 0.8774036169052124\n",
      "Epoch 6 Batch 1600 Loss 1.0590808391571045\n",
      "Epoch 6 Batch 1800 Loss 0.8688045740127563\n",
      "Epoch 6 Batch 2000 Loss 0.9616774320602417\n",
      "Epoch 6 Batch 2200 Loss 0.7977191209793091\n",
      "Epoch 6 Batch 2400 Loss 0.98979252576828\n",
      "Epoch 6 Loss 0.9987\n",
      "Time taken for epoch 333.5188343524933 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 1.2122323513031006\n",
      "Epoch 7 Batch 200 Loss 0.8854308128356934\n",
      "Epoch 7 Batch 400 Loss 0.9393531680107117\n",
      "Epoch 7 Batch 600 Loss 0.9358025789260864\n",
      "Epoch 7 Batch 800 Loss 0.9645635485649109\n",
      "Epoch 7 Batch 1000 Loss 1.0512157678604126\n",
      "Epoch 7 Batch 1200 Loss 0.9283155202865601\n",
      "Epoch 7 Batch 1400 Loss 0.9474112391471863\n",
      "Epoch 7 Batch 1600 Loss 1.0158908367156982\n",
      "Epoch 7 Batch 1800 Loss 1.0779320001602173\n",
      "Epoch 7 Batch 2000 Loss 1.0868278741836548\n",
      "Epoch 7 Batch 2200 Loss 0.7850838899612427\n",
      "Epoch 7 Batch 2400 Loss 0.9772783517837524\n",
      "Epoch 7 Loss 0.9976\n",
      "Time taken for epoch 332.555903673172 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 1.1519498825073242\n",
      "Epoch 8 Batch 200 Loss 0.8861268758773804\n",
      "Epoch 8 Batch 400 Loss 0.9910993576049805\n",
      "Epoch 8 Batch 600 Loss 1.0419161319732666\n",
      "Epoch 8 Batch 800 Loss 1.0065147876739502\n",
      "Epoch 8 Batch 1000 Loss 1.061415672302246\n",
      "Epoch 8 Batch 1200 Loss 0.888361930847168\n",
      "Epoch 8 Batch 1400 Loss 1.0273559093475342\n",
      "Epoch 8 Batch 1600 Loss 1.1724306344985962\n",
      "Epoch 8 Batch 1800 Loss 1.0431936979293823\n",
      "Epoch 8 Batch 2000 Loss 1.072453260421753\n",
      "Epoch 8 Batch 2200 Loss 0.8340465426445007\n",
      "Epoch 8 Batch 2400 Loss 0.9229194521903992\n",
      "Epoch 8 Loss 1.0821\n",
      "Time taken for epoch 332.4956159591675 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 1.1275014877319336\n",
      "Epoch 9 Batch 200 Loss 0.9886582493782043\n",
      "Epoch 9 Batch 400 Loss 1.2485461235046387\n",
      "Epoch 9 Batch 600 Loss 1.2668871879577637\n",
      "Epoch 9 Batch 800 Loss 1.3365728855133057\n",
      "Epoch 9 Batch 1000 Loss 1.4723541736602783\n",
      "Epoch 9 Batch 1200 Loss 1.371246576309204\n",
      "Epoch 9 Batch 1400 Loss 1.719102144241333\n",
      "Epoch 9 Batch 1600 Loss 1.7484090328216553\n",
      "Epoch 9 Batch 1800 Loss 1.8107540607452393\n",
      "Epoch 9 Batch 2000 Loss 1.7928138971328735\n",
      "Epoch 9 Batch 2200 Loss 1.8857147693634033\n",
      "Epoch 9 Batch 2400 Loss 1.7977087497711182\n",
      "Epoch 9 Loss 1.7782\n",
      "Time taken for epoch 334.0803816318512 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 1.8541167974472046\n",
      "Epoch 10 Batch 200 Loss 1.8903169631958008\n",
      "Epoch 10 Batch 400 Loss 2.0889225006103516\n",
      "Epoch 10 Batch 600 Loss 2.208070755004883\n",
      "Epoch 10 Batch 800 Loss 1.9546020030975342\n",
      "Epoch 10 Batch 1000 Loss 2.028101921081543\n",
      "Epoch 10 Batch 1200 Loss 1.7229516506195068\n",
      "Epoch 10 Batch 1400 Loss 1.9513099193572998\n",
      "Epoch 10 Batch 1600 Loss 2.073265790939331\n",
      "Epoch 10 Batch 1800 Loss 2.023165464401245\n",
      "Epoch 10 Batch 2000 Loss 1.855939507484436\n",
      "Epoch 10 Batch 2200 Loss 1.8375511169433594\n",
      "Epoch 10 Batch 2400 Loss 1.880196213722229\n",
      "Epoch 10 Loss 1.8228\n",
      "Time taken for epoch 333.96486830711365 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 1.828315019607544\n",
      "Epoch 11 Batch 200 Loss 1.8609542846679688\n",
      "Epoch 11 Batch 400 Loss 1.8450860977172852\n",
      "Epoch 11 Batch 600 Loss 2.0816798210144043\n",
      "Epoch 11 Batch 800 Loss 1.85468327999115\n",
      "Epoch 11 Batch 1000 Loss 1.9184490442276\n",
      "Epoch 11 Batch 1200 Loss 1.7428697347640991\n",
      "Epoch 11 Batch 1400 Loss 1.8419872522354126\n",
      "Epoch 11 Batch 1600 Loss 1.8686004877090454\n",
      "Epoch 11 Batch 1800 Loss 1.7323917150497437\n",
      "Epoch 11 Batch 2000 Loss 1.7236019372940063\n",
      "Epoch 11 Batch 2200 Loss 1.8537302017211914\n",
      "Epoch 11 Batch 2400 Loss 1.7244453430175781\n",
      "Epoch 11 Loss 1.7736\n",
      "Time taken for epoch 332.40935587882996 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 1.8229478597640991\n",
      "Epoch 12 Batch 200 Loss 1.841168999671936\n",
      "Epoch 12 Batch 400 Loss 2.017482042312622\n",
      "Epoch 12 Batch 600 Loss 2.098597288131714\n",
      "Epoch 12 Batch 800 Loss 1.9587510824203491\n",
      "Epoch 12 Batch 1000 Loss 2.015594959259033\n",
      "Epoch 12 Batch 1200 Loss 1.8014228343963623\n",
      "Epoch 12 Batch 1400 Loss 1.8154914379119873\n",
      "Epoch 12 Batch 1600 Loss 1.9469387531280518\n",
      "Epoch 12 Batch 1800 Loss 1.8235899209976196\n",
      "Epoch 12 Batch 2000 Loss 1.949449896812439\n",
      "Epoch 12 Batch 2200 Loss 1.7968896627426147\n",
      "Epoch 12 Batch 2400 Loss 1.819340467453003\n",
      "Epoch 12 Loss 1.7596\n",
      "Time taken for epoch 332.0481495857239 sec\n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints4'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "# Training step\n",
    "EPOCHS = 12\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    # initializing the hidden state at the start of every epoch\n",
    "    # initally hidden is None\n",
    "    hidden = model.reset_states()\n",
    "\n",
    "    #for (batch_n, (inp, target)) in enumerate(datasets[0]):\n",
    "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
    "        loss = train_step(inp, target)\n",
    "        \n",
    "        if batch_n % 200 == 0:\n",
    "            template = 'Epoch {} Batch {} Loss {}'\n",
    "            print(template.format(epoch+1, batch_n, loss))\n",
    "\n",
    "    # saving (checkpoint) the model every 5 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
    "\n",
    "    print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
    "    print ('Time taken for epoch {} sec\\n'.format(time.time() - start))\n",
    "\n",
    "print(\"Done\")\n",
    "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./training_checkpoints4/ckpt_11\n"
     ]
    }
   ],
   "source": [
    "print(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "#'./training_checkpoints2/ckpt_6'\n",
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            554496    \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (1, None, 1024)           3938304   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 2166)           2220150   \n",
      "=================================================================\n",
      "Total params: 6,712,950\n",
      "Trainable params: 6,712,950\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    # Evaluation step (generating text using the learned model)\n",
    "\n",
    "    # Number of characters to generate\n",
    "    num_generate = 2000\n",
    "\n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "    temperature = 0.8\n",
    "\n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # using a categorical distribution to predict the character returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "        \n",
    "        # We pass the predicted character as the next input to the model\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating\n",
      "<html\"></p>\n",
      "</ class=\"nam\">69 0 112 0 27 3 col-rever-colums-traner=\"trangrypes/div.proff\"},\"styleIded\":1.699,9,0,0,l, ===========as.tite-49c9u9 4.77186 0-111-13-10-7l-24-2-13 94.7-70-1-1-2.0 zl -->                         <ul>\n",
      "                                 <div class=\"fo-xworlay\" ta></div>\n",
      "</div></div></div>\n",
      "</div></div></div>\n",
      "</div>\n",
      "</div>\n",
      "</div>\n",
      "</div>\n",
      "<div class=\"empont\"><a href=\"/js\",\"data-iage: roode-911la00029\"><a href=\"https://www.wiht.products.ae/2018/00017/04-22-26-15-32-243f-129-712-834-43-313-2.5146-12-had-11-4.0-1.0'></style>\n",
      "</div>\n",
      "</div><bar class='trefublogta-fightle-wrapplect-slis.data-ed-tp-content/themontenter\"></i>\n",
      "\t\t\t\n",
      "\t\t\t\t\t\t<div class=\"fa-f0-20-9 51-1-1 8-4 0-2-48-61-58-120-1 4-2 59-5ec-2122 20 22-327 71.0 2-13-2 2-4 4-17-2.<se-content/pemarthting-height\"><il class=\"ims-453-343-44 -7-1-7-112-2 13-9 63 5 63 min-sitem-colation: 000;sX-483-300 0 5541 248 0 0 84 columnynery-15px 0 1 2 63 23 3z-1-14 4-333-4 8 class=\"spost_type\">\n",
      "<div class=\"col-md-620-4\"><lia                                                                                                                                                                                                                                                                                                                                                     <a href=\"https:///stay.css?ver=\"corter\":\"priew\",\"modum-buttor-itexicaile-struee-topal\"><div class=\"3\"                                                                                 font-src=\"https://www.wist.com/stingstaine-edmetemprent\">\n",
      "         <div class=\"s3\"link\"\">\n",
      "\t\t<aut the=\"host_lestimen\">\n",
      "<div class=\"foll-services/sproducaltform/in/\"></avall>\n",
      "<div class=imation: falsboxee2\n",
      "                                                                                           </div>\n",
      "\t\t\t\t\t\t\t\t\t\t<li >\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
      "\t\t\t<div class=\"color: 0 115 0 0 4-568212 79.8 1 0 1 1-3 1 16 0 19 1.69 41.24                                                              </symonts/fonts/bod\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating\")\n",
    "print(generate_text(model, start_string=u\"<html\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
